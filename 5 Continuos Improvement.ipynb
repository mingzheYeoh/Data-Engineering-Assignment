{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d55acc-4bc6-44e5-9fd6-6e9ce2dc04ee",
   "metadata": {},
   "source": [
    "<h1>Continuous Improvement</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399e13b-7b47-4ec3-b4a4-95bb6ec5ba36",
   "metadata": {},
   "source": [
    "<h3>Connect to MongoDB Cloud database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1276ca-ac14-45da-8564-a7467fee9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.pymongo_utils import PyMongoUtils\n",
    "uri = \"mongodb+srv://eamon:eamontest123@cluster0.imvj5.mongodb.net/lexicon?retryWrites=true&w=majority\"\n",
    "database_name = \"lexicon\"\n",
    "collection_name = \"annotated_words\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f801e-c508-4e47-9754-39b687bf06da",
   "metadata": {},
   "source": [
    "<h3>Task 1 CSV Comparison Method</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469da776-df1c-41a1-b284-2399ccae1cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/de-prj/de-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "24/12/12 21:21:16 WARN Utils: Your hostname, MSI. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/12 21:21:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/12 21:21:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|      hingga|    6|\n",
      "|        rumi|    3|\n",
      "|      nombor|    4|\n",
      "|   perkiraan|    3|\n",
      "|        dari|   11|\n",
      "|        juga|    4|\n",
      "|       dalam|    9|\n",
      "|       lihat|    4|\n",
      "|       tahun|   24|\n",
      "|      kepada|    5|\n",
      "|          an|    6|\n",
      "|     merujuk|    3|\n",
      "|          sm|    9|\n",
      "|       huruf|    3|\n",
      "|       falak|    3|\n",
      "|       boleh|    3|\n",
      "|   pemilihan|    1|\n",
      "|antarabangsa|    1|\n",
      "|       dewan|    2|\n",
      "|bagaimanapun|    1|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "Redis database has been flushed.\n",
      "Insertion into Redis completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           word|\n",
      "+---------------+\n",
      "|       perayaan|\n",
      "|      pemilihan|\n",
      "|        noricum|\n",
      "|          dewan|\n",
      "|         jerman|\n",
      "|      peristiwa|\n",
      "|         yahudi|\n",
      "|       tenggara|\n",
      "|         profil|\n",
      "|    dioscorides|\n",
      "|  diperkenalkan|\n",
      "|      kesibukan|\n",
      "|penyelesaiannya|\n",
      "|           puak|\n",
      "|         yunani|\n",
      "|      perubatan|\n",
      "|           tiga|\n",
      "|     empayarnya|\n",
      "|    kemerdekaan|\n",
      "|        adiknya|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in DataFrame: 289\n"
     ]
    }
   ],
   "source": [
    "from classes.csvUpdate import csvUpdateUtils\n",
    "from classes.SparkBuilder import sparkBuilder\n",
    "from classes.LexiconCreator import lexiconRawDataCollector\n",
    "from classes.LexiconAnnotater import lexiconAnnotater\n",
    "\n",
    "sB = sparkBuilder()\n",
    "lexicon = lexiconRawDataCollector()\n",
    "annotater = lexiconAnnotater()\n",
    "spark = sB.sparkSessionBuilder(\"Lexicon CSV Updater\")\n",
    "#scarp new data from wikipedia, clean the data, and take data out from storageCleanedCSV \n",
    "csvDate = \"12.12.2024_21.08.30\"\n",
    "refresherName = \"LWM\"\n",
    "csvUtils = csvUpdateUtils()\n",
    "dfNewWord = csvUtils.compareToUpdateCSV(spark, csvDate,refresherName, uri, database_name, collection_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207e0a2-2d81-4fe7-a575-59b66d8e43a8",
   "metadata": {},
   "source": [
    "<h4>1.1 Now we export the csv file for annotation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443f02a4-75f0-436d-beb6-27cfb35a074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words with frequencies saved to storageContinuousUpdateCSV/updateCSVOf_12.12.2024_21.21.30_LWM.csv\n"
     ]
    }
   ],
   "source": [
    "#Export to CSV\n",
    "csvUtils.exportUpdateCSV(dfNewWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a390c-19c4-44ae-9b61-0c73697ccc86",
   "metadata": {},
   "source": [
    "<h4>After the annotation is done, import back in.</h4>\n",
    "<p>Two ways users can do the annotation for the sentiment labeling part.\n",
    "<br><br>\n",
    "The first one would be utilizing the function that has been provided at the notebook <b>3. Sentiment Labeling</b> that label the words with sentiment using huggingface model, the csv file annotated would be saved at \"storageLabeledUpdateCSV\" folder, and you can manual annotate the rest in the folder. <b>(Please keep the filename and path the same!)</b>\n",
    "<br><br>\n",
    "The second one would be fully manual annotation or using another model that is not provided here. This, u can take the data file out on the storageContinuousUpdateCSV folder and look for the respective file u have exported, annotate using your own ways and put it back in to the original file <b>(Please keep the filename and path the same!)</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699419e-d129-4878-bd4a-f429b7728867",
   "metadata": {},
   "source": [
    "<h4>Then we can now pass in the value into the MongoDB database.</h4>\n",
    "<p>\n",
    "Consistency is ensured as the reinsertion of MongoDB data into the redis (which is the compareable agent is being refresh every time any changes has been done.)\n",
    "<br><br>\n",
    "Now here you would need to import the csv file based on whether the first or second option is selected.\n",
    "<br><br>\n",
    "If first option is selected, choose 1 for the \"scenario\" variable.\n",
    "<br><br>\n",
    "If second option is selected, choose 2 for the \"scenario\" variable.\n",
    "<br><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30503f6-0bb3-46cf-809a-2af6802e671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis database has been flushed.\n",
      "Insertion into Redis completed.\n",
      "+---------+-----+---------+---------------+\n",
      "|     word|count|sentiment|sentiment_score|\n",
      "+---------+-----+---------+---------------+\n",
      "| perayaan|    1| positive|    0.927380264|\n",
      "|pemilihan|    1|  neutral|            0.5|\n",
      "+---------+-----+---------+---------------+\n",
      "\n",
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- sentiment_score: double (nullable = true)\n",
      "\n",
      "Inserted 2 documents into MongoDB.\n",
      "Redis database has been flushed.\n",
      "Insertion into Redis completed.\n"
     ]
    }
   ],
   "source": [
    "#Import to MongoDB\n",
    "scenario = 1\n",
    "csvDateFileWanted = \"12.12.2024_21.15.25\"\n",
    "refresherName = \"LWM\"\n",
    "csvUtils.importUpdatedCSVToMongo(spark, scenario, csvDateFileWanted, refresherName, uri, database_name, collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0d60d-ec45-43a1-a0c5-992ed9e97c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sB.sparkStopSession(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4475ab1-40fd-4876-8bcc-ee8172735f63",
   "metadata": {},
   "source": [
    "<h3>Task 2 Manual Insertion</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcce96-7ecf-48bd-bb27-995d59eecd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from classes.wordManipulator import wordManipulator\n",
    "#wordMani = wordManipulator()\n",
    "#newWord = input(\"Enter a word to key in: \").strip()\n",
    "#scenario = \"newWord\"\n",
    "#wordMani.wordInput(newWord, scenario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ass-venv",
   "language": "python",
   "name": "ass-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
